{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxEdAOYX-Ext"
      },
      "source": [
        "# Семинар 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "\n",
        "if \"google.colab\" in sys.modules and not os.path.exists(\".setup_complete\"):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ[\"DISPLAY\"] = \":1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uVrCHTToqeD",
        "outputId": "41c1c979-0e8f-4e9b-b390-156fafb0f0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXJy0PNb-Exw"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque, defaultdict\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabular Q-learning"
      ],
      "metadata": {
        "id": "Oagb1LHzq_-F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx5RcuOyrB-m"
      },
      "source": [
        "Реализуем алгоритм Q-learning для среды CliffWalking. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs import toy_text"
      ],
      "metadata": {
        "id": "LqgPC4qetbJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPhZNmd4rB-m"
      },
      "source": [
        "env = toy_text.CliffWalkingEnv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=16nbZM2ZoY7xabN4uDEvCCz9IWmk_9D_x)"
      ],
      "metadata": {
        "id": "S1ZEyPCVt86g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JHUoKyfrB-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb15b52-26dd-44a1-ed12-42529885b809"
      },
      "source": [
        "print(env.__doc__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    This is a simple implementation of the Gridworld Cliff\n",
            "    reinforcement learning task.\n",
            "\n",
            "    Adapted from Example 6.6 (page 106) from Reinforcement Learning: An Introduction\n",
            "    by Sutton and Barto:\n",
            "    http://incompleteideas.net/book/bookdraft2018jan1.pdf\n",
            "\n",
            "    With inspiration from:\n",
            "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
            "\n",
            "    The board is a 4x12 matrix, with (using Numpy matrix indexing):\n",
            "        [3, 0] as the start at bottom-left\n",
            "        [3, 11] as the goal at bottom-right\n",
            "        [3, 1..10] as the cliff at bottom-center\n",
            "\n",
            "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
            "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWAniYcrB-m"
      },
      "source": [
        "Так как награда выражатеся отрицательными значениями, то фактической целью агента является как можно более быстрое преодоление пути от старта к финишу при этом ему нужно не упасть с обрыва."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AalEO1PfrB-m"
      },
      "source": [
        "Традиционно посмотрим сколько очков в среднем за 100 эпизодов сможет набрать \"cлучайный\" агент."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQjmQwAsrB-n"
      },
      "source": [
        "total_reward = []\n",
        "for episode in range(10):\n",
        "    \n",
        "    for t in range(100):\n",
        "        \n",
        "        if done:\n",
        "            print(\"Episode {} finished after {} timesteps\".format(episode+1, t+1))\n",
        "            break\n",
        "    total_reward.append(episode_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsTj3pj4rB-n"
      },
      "source": [
        "print(np.mean(total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seJZ2PVkrB-n"
      },
      "source": [
        "Ожидаемо, наш \"случайный\" агент просто блуждает по среде, не пытаясь добраться до цели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc5OmdQDrB-n"
      },
      "source": [
        "Вспомним как выглядит реализация данного алгоритма \n",
        "![](https://drive.google.com/uc?export=view&id=1eMG5HUaDyKKE8DQUhEdOaBZVR1UqOTO9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkHnQ5sjrB-n"
      },
      "source": [
        "Так как мы раелизуем табличную версию Q-learning, создадим структура, которая будет хранить значения нашей функции *Q(S,A)* для каждого состояния и действия. Она пдетставляет собой словарь (*dict*), хранящий в качестве ключей состояния, а в качестве значений массив значений *Q-функции* для каждого действия для данного ключа-состояния."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SowXKlJXrB-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620695a2-8122-4546-fd3d-9a7a87cea927"
      },
      "source": [
        "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "print(\"Q-значения для состояния-действия (0, 0): %s\" % Q[(0, 0)], \"хранятся в списке, по значению для каждого действия\")\n",
        "print(\"Таким обарзом, Q-значение для действия 3 в в состоянии (1,2), i.e. Q((1,2), 3), можно получить вот так q_vals[(1,2)][3]:\", Q[(1,2)][3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-значения для состояния-действия (0, 0): [0. 0. 0. 0.] хранятся в списке, по значению для каждого действия\n",
            "Таким обарзом, Q-значение для действия 3 в в состоянии (1,2), i.e. Q((1,2), 3), можно получить вот так q_vals[(1,2)][3]: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJzC5jvIrB-n"
      },
      "source": [
        "Сначала напишем функцию реализующую $\\epsilon$-greedy политику для исследования среды."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbSZzeJgrB-o"
      },
      "source": [
        "def eps_greedy(Q, state, epsilon):\n",
        "    \"\"\"\n",
        "    Параметры:\n",
        "        Q: таблица значений Q-функции\n",
        "        epsilon: параметр эпсилон\n",
        "        state: текущее состоние\n",
        "    Результат:\n",
        "        Случайное действие с вероятностью eps или argmax Q(s, .) c вероятностью (1 - eps)\n",
        "    \"\"\"\n",
        "    # To Do\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNvNjx8urB-o"
      },
      "source": [
        "Также напишем функцию, которая поможет нам оценить, как ведет себя обученный агент."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6J3bywzrB-o"
      },
      "source": [
        "def q_eval(Q, render=False):\n",
        "    total_reward = []\n",
        "    for i_episode in range(10):\n",
        "        episode_reward = 0\n",
        "        observation = env.reset()\n",
        "        for t in range(100):\n",
        "            \n",
        "            \n",
        "            \n",
        "            observation, reward, done, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "\n",
        "        total_reward.append(episode_reward)\n",
        "    return np.mean(total_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et1UkprirB-o"
      },
      "source": [
        "Теперь реализуем функцию обучения агента (обновления значений Q-функции) с использование функции, реализующей $\\epsilon$-greedy политику."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pmiwWqnrB-o"
      },
      "source": [
        "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
        "    \"\"\"    \n",
        "    Параменты:\n",
        "        env: среда Open AI\n",
        "        num_episodes: Количество эпизодов\n",
        "        discount_factor: фактор дисконтирования\n",
        "        alpha: константа обучения\n",
        "        epsilon: параметр эпсилон для ϵ-greedy политику\n",
        "    \n",
        "    Результат:\n",
        "        Таблица с оптимальными значениями Q-функции\n",
        "    \"\"\"\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    episode_lengths=np.zeros(num_episodes)\n",
        "    episode_rewards=np.zeros(num_episodes)\n",
        "        \n",
        "    for i_episode in range(num_episodes):\n",
        "        if (i_episode + 1) % 100 == 0:\n",
        "            print(\"\\rEpisode {}/{} score = {}.\".format(i_episode + 1, num_episodes, q_eval(Q)))\n",
        "        \n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        len = 0\n",
        "        # To Do\n",
        "          \n",
        "        # While episode is not over\n",
        "\n",
        "    \n",
        "    return Q, episode_lengths, episode_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wFpiP-_rB-o"
      },
      "source": [
        "Q, episode_lengths, episode_rewards = q_learning(env, 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gtlY0jLrB-p"
      },
      "source": [
        "score = q_eval(Q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDbZrAcxrB-p"
      },
      "source": [
        "plt.plot(episode_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLzy87b9rB-p"
      },
      "source": [
        "plt.plot(episode_rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "rgFSL4zGrA29"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYZyWoXy-Exy"
      },
      "source": [
        "## Q-learning algorithm\n",
        "\n",
        "Наша цель будет состоять в обучении политике, которая пытается максимизировать дисконтированные,\n",
        "накопительное вознаграждение\n",
        "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$\n",
        "\n",
        "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$\n",
        "\n",
        "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
        "\n",
        "Обновляем Q функцию исходя из того, что политика должна удовлетворять уравнению Беллмана\n",
        "\n",
        "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
        "\n",
        "Разницу между правой и левой частями уравнения называем temporal difference error, $\\delta$:\n",
        "\n",
        "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
        "\n",
        "Именно эту ошибку мы и будем мимнимизировать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCRPSMFr-Exz"
      },
      "source": [
        "## Будем работать непосредственно с экраном игры, для этого объявим вспомогательные функции"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0').unwrapped"
      ],
      "metadata": {
        "id": "L-LpSTsLpYD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2gl1Y_3-Exz"
      },
      "source": [
        "Преобразование картинки в тензор"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-08a1Fl-Ex0",
        "outputId": "2b9fdc7c-a3d1-4267-fb9b-a5b118818139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        }
      ],
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl3A5g2U-Ex0"
      },
      "source": [
        "Считывание положения каретки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnLAqPSc-Ex1"
      },
      "outputs": [],
      "source": [
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UleAin-H-Ex1"
      },
      "source": [
        "Считывание экрана игры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbWB2SPa-Ex1"
      },
      "outputs": [],
      "source": [
        "def get_screen():\n",
        "    # gym возвращает экран с размером 400x600x3\n",
        "    # Transpose it into torch order (CHW).\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    \n",
        "    # Обрежем верх и низ картинки\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    \n",
        "    # Центрируем картинку по каретке\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "        \n",
        "    # Обрезаем лишнюю часть, чтобы получить квадратную область\n",
        "    screen = screen[:, :, slice_range]\n",
        "    \n",
        "    # Переводим картинку в тензор\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    \n",
        "    # Сжимаем картинку и добавляем размерность батча\n",
        "    return resize(screen).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVBjrRrP-Ex2"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ7VnrRw-Exx"
      },
      "source": [
        "## Replay Memory\n",
        "\n",
        "Мы будем использовать воспроизведения опыта для тренировки нашей DQN. Replay Memory хранит\n",
        "переходы, которые наблюдает агент, что позволяет нам повторно использовать эти данные\n",
        "потом. Для уменьшения корреляций между состояниями и переходами мы будем выбирать случайные переходы из предыдущих игр."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhaE0p4h-Exx"
      },
      "outputs": [],
      "source": [
        "# Создаем кортежи, в которых будем хранить наш опыт\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# Создаем буфер, в котором будем хранить наш опыт\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "       self.memory = deque([],maxlen=capacity)        \n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        self.memory.append(Transition(*args))        \n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7Ple45u-Exy"
      },
      "source": [
        "## Создадим модель для апроксимации политики"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncHawd7v-Exy"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "      super(DQN, self).__init__()\n",
        "\n",
        "\n",
        "      def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "          return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      return self.head(x.view(x.size(0), -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZHRSJRd-Ex2"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH_eVbT5-Ex2"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 400\n",
        "TARGET_UPDATE = 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1500):\n",
        "  print(math.exp(-1. * i / EPS_DECAY))"
      ],
      "metadata": {
        "id": "y1Mr0-fq_BnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvI4ESAk-Ex2"
      },
      "outputs": [],
      "source": [
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "n_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lirg7Y4n-Ex3"
      },
      "source": [
        "Объявляем две модели, одна отвечает за политику во время игры, другая для предсказания награды"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zx2Zwmr-Ex3"
      },
      "outputs": [],
      "source": [
        "policy_net = \n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(100000)\n",
        "\n",
        "episode_durations = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps_done = 0"
      ],
      "metadata": {
        "id": "bIJs8qvtmjkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpWYGZGU-Ex3"
      },
      "source": [
        "Выбор действия"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBXjZccz-Ex3"
      },
      "outputs": [],
      "source": [
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    print(eps_threshold)\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwfY2a3Y-Ex3"
      },
      "source": [
        "Визуализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnNDg9ne-Ex4"
      },
      "outputs": [],
      "source": [
        "def plot_durations():\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 30:\n",
        "        means = durations_t.unfold(0, 30, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(29), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.0001)  # pause a bit so that plots are updated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik_fUVH1-Ex4"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCUR5LOl-Ex4"
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    \n",
        "    # batch-array of Transitions to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Парсим transitions чтобы сформировать обучающую выборку\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Вычисляем Q(s_t, a) - Модель предсказывает Q(s_t), \n",
        "    # затем мы выбираем Q, соответствующие сделанному действию\n",
        "    state_action_values = #\n",
        "\n",
        "    # Вычисляем V(s_{t+1}) для всех следующих состояний target_net(non_final_next_states)\n",
        "    # берем максимальной значение\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = #\n",
        "    \n",
        "    # Вычисляем ожидаемые значения Q функции\n",
        "    expected_state_action_values = #\n",
        "\n",
        "    # Вычисляем Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion()\n",
        "\n",
        "    # Шаг оптимизации\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #for param in policy_net.parameters():\n",
        "    #    param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9shX622-Ex4"
      },
      "source": [
        "## Основной цикл обучения\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFpl6MsJ-Ex4"
      },
      "outputs": [],
      "source": [
        "num_episodes = 50\n",
        "for i_episode in range(num_episodes):\n",
        "    \n",
        "    # Инициализация\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen    \n",
        "    \n",
        "    for t in count():\n",
        "        \n",
        "        # Выбираем действие\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # Наблюдаем новое состояние\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # Сохраняем transition в буфер\n",
        "        memory.push(state, action, next_state, reward)\n",
        "        state = next_state\n",
        "\n",
        "        # Делаем шаг оптимизации\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "            \n",
        "    # Обновляем target network, копирую веса из policy_net\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output, display\n",
        "import time\n",
        "\n",
        "env.reset()\n",
        "last_screen = get_screen()\n",
        "current_screen = get_screen()\n",
        "state = current_screen - last_screen    \n",
        "\n",
        "time_limit = 80\n",
        "for time_step in range(time_limit):\n",
        "    # Choose action based on your policy.\n",
        "    action = policy_net(state).max(1)[1].view(1, 1)\n",
        "    _, reward, done, _ = env.step(action.item())\n",
        "    print(action.item())\n",
        "\n",
        "    # Наблюдаем новое состояние\n",
        "    last_screen = current_screen\n",
        "    current_screen = get_screen()\n",
        "    if not done:\n",
        "        next_state = current_screen - last_screen\n",
        "    else:\n",
        "        next_state = None\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    # Draw game image on display.\n",
        "    clear_output(wait=True)\n",
        "    plt.title(time_step)\n",
        "    plt.imshow(env.render(\"rgb_array\"))\n",
        "    plt.show()\n",
        "    time.sleep(.5)\n",
        "\n",
        "    if time_step > 79:\n",
        "        print(\"Well done!\")\n",
        "        break\n",
        "\n",
        "    if done:\n",
        "        print(\"you lose\")\n",
        "        break"
      ],
      "metadata": {
        "id": "od9QOWO26k4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KBe3CGO68ze1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Seminar_10_QLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}