{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "hw_2_trees.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfqZMCHsgCtS"
      },
      "source": [
        "# Decision Tree construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLUYW55BgCtz"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.datasets import make_classification, make_regression, load_digits, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8szv9FsggCtz"
      },
      "source": [
        "Let's fix the `random_state` (a.k.a. random seed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZd813HIgCtz"
      },
      "source": [
        "RANDOM_STATE = 42"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crZUBzd8gCt0"
      },
      "source": [
        "__Your ultimate task for today is to impement the `DecisionTree` class and use it to solve classification and regression problems.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCbGfFk8i4Od"
      },
      "source": [
        "## __Specifications:__\n",
        "- The class inherits from `sklearn.BaseEstimator`;\n",
        "- Constructor is implemented for you. It has the following parameters:\n",
        "    * `max_depth` - maximum depth of the tree; `np.inf` by default\n",
        "    * `min_samples_split` - minimal number of samples in the leaf to make a split; `2` by default;\n",
        "    * `criterion` - criterion to select the best split; in classification one of `['gini', 'entropy']`, default `gini`; in regression `variance`;\n",
        "\n",
        "- `fit` method takes `X` (`numpy.array` of type `float` shaped `(n_objects, n_features)`) and `y` (`numpy.array` of type float shaped `(n_objects, 1)` in regression; `numpy.array` of type int shaped `(n_objects, 1)` with class labels in classification). It works inplace and fits the `DecisionTree` class instance to the provided data from scratch.\n",
        "\n",
        "- `predict` method takes `X` (`numpy.array` of type `float` shaped `(n_objects, n_features)`) and returns the predicted $\\hat{y}$ values. In classification it is a class label for every object (the most frequent in the leaf; if several classes meet this requirement select the one with the smallest class index). In regression it is the desired constant (e.g. mean value for `variance` criterion)\n",
        "\n",
        "- `predict_proba` method (works only for classification (`gini` or `entropy` criterion). It takes `X` (`numpy.array` of type `float` shaped `(n_objects, n_features)`) and returns the `numpy.array` of type `float` shaped `(n_objects, n_features)` with class probabilities for every object from `X`. Class $i$ probability equals the ratio of $i$ class objects that got in this node in the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7Kbubcbifdt"
      },
      "source": [
        "## __Small recap:__\n",
        "\n",
        "To find the optimal split the following functional is evaluated:\n",
        "    \n",
        "$$G(j, t) = H(Q) - \\dfrac{|L|}{|Q|} H(L) - \\dfrac{|R|}{|Q|} H(R),$$\n",
        "    where $Q$ is the dataset from the current node, $L$ and $R$ are left and right subsets defined by the split $x^{(j)} < t$.\n",
        "\n",
        "\n",
        "\n",
        "1. Classification. Let $p_i$ be the probability of $i$ class in subset $X$ (ratio of the $i$ class objects in the dataset). The criterions are defined as:\n",
        "    \n",
        "    * `gini`: Gini impurity $$H(R) = 1 -\\sum_{i = 1}^K p_i^2$$\n",
        "    \n",
        "    * `entropy`: Entropy $$H(R) = -\\sum_{i = 1}^K p_i \\log(p_i)$$ (One might use the natural logarithm).\n",
        "    \n",
        "2. Regression. Let $y_l$ be the target value for the $R$, $\\mathbf{y} = (y_1, \\dots, y_N)$ â€“ all targets for the selected dataset $X$.\n",
        "    \n",
        "    * `variance`: $$H(R) = \\dfrac{1}{|R|} \\sum_{y_j \\in R}(y_j - \\text{mean}(\\mathbf{y}))^2$$\n",
        "    \n",
        "    * `mad_median`: $$H(R) = \\dfrac{1}{|R|} \\sum_{y_j \\in R}|y_j - \\text{median}(\\mathbf{y})|$$\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSnX4UN-gCt1"
      },
      "source": [
        "**Hints and comments**:\n",
        "\n",
        "* No need to deal with categorical features, they will not be present.\n",
        "* Siple greedy recursive procedure is enough. However, you can speed it up somehow (e.g. using percentiles).\n",
        "* Please, do not copy implementations available online. You are supposed to build very simple example of the Decision Tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWpfAUgli6tU"
      },
      "source": [
        "## __Realisation__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbksbwW9hysO"
      },
      "source": [
        "def entropy(y):  \n",
        "    \"\"\"\n",
        "    Computes entropy of the provided distribution. Use log(value + eps) for numerical stability\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.array of type float with shape (n_objects, n_classes)\n",
        "        One-hot representation of class labels for corresponding subset\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Entropy of the provided subset\n",
        "    \"\"\"\n",
        "    EPS = 0.0005\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    return 0."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDyHnjL5h1dG"
      },
      "source": [
        "def gini(y):\n",
        "    \"\"\"\n",
        "    Computes the Gini impurity of the provided distribution\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.array of type float with shape (n_objects, n_classes)\n",
        "        One-hot representation of class labels for corresponding subset\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Gini impurity of the provided subset\n",
        "    \"\"\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    return 0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoLuz4iWh1f1"
      },
      "source": [
        "def variance(y):\n",
        "    \"\"\"\n",
        "    Computes the variance the provided target values subset\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.array of type float with shape (n_objects, 1)\n",
        "        Target values vector\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Variance of the provided target vector\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    return 0.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va68QiFch1id"
      },
      "source": [
        "def mad_median(y):\n",
        "    \"\"\"\n",
        "    Computes the mean absolute deviation from the median in the\n",
        "    provided target values subset\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.array of type float with shape (n_objects, 1)\n",
        "        Target values vector\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Mean absolute deviation from the median in the provided vector\n",
        "    \"\"\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    return 0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KvuQQ04h1lH"
      },
      "source": [
        "def one_hot_encode(n_classes, y):\n",
        "    y_one_hot = np.zeros((len(y), n_classes), dtype=float)\n",
        "    y_one_hot[np.arange(len(y)), y.astype(int)[:, 0]] = 1.\n",
        "    return y_one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds0DHjRRh1nn"
      },
      "source": [
        "def one_hot_decode(y_one_hot):\n",
        "    return y_one_hot.argmax(axis=1)[:, None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYNbSqfIh1qP"
      },
      "source": [
        "class Node:\n",
        "    \"\"\"\n",
        "    This class is provided \"as is\" and it is not mandatory to it use in your code.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_index, threshold, proba=0):\n",
        "        self.feature_index = feature_index\n",
        "        self.value = threshold\n",
        "        self.proba = proba\n",
        "        self.left_child = None\n",
        "        self.right_child = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXTiCKQAiAvo"
      },
      "source": [
        "class DecisionTree(BaseEstimator):\n",
        "    all_criterions = {\n",
        "        'gini': (gini, True), # (criterion, classification flag)\n",
        "        'entropy': (entropy, True),\n",
        "        'variance': (variance, False),\n",
        "        'mad_median': (mad_median, False)\n",
        "    }\n",
        "\n",
        "    def __init__(self, n_classes=None, max_depth=np.inf, min_samples_split=2, \n",
        "                 criterion_name='gini', debug=False):\n",
        "\n",
        "        assert criterion_name in self.all_criterions.keys(), 'Criterion name must be on of the following: {}'.format(self.all_criterions.keys())\n",
        "        \n",
        "        self.n_classes = n_classes\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion_name = criterion_name\n",
        "\n",
        "        self.depth = 0\n",
        "        self.root = None # Use the Node class to initialize it later\n",
        "        self.debug = debug\n",
        "\n",
        "        \n",
        "        \n",
        "    def make_split(self, feature_index, threshold, X_subset, y_subset):\n",
        "        \"\"\"\n",
        "        Makes split of the provided data subset and target values using provided feature and threshold\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        feature_index : int\n",
        "            Index of feature to make split with\n",
        "\n",
        "        threshold : float\n",
        "            Threshold value to perform split\n",
        "\n",
        "        X_subset : np.array of type float with shape (n_objects, n_features)\n",
        "            Feature matrix representing the selected subset\n",
        "\n",
        "        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n",
        "                   (n_objects, 1) in regression \n",
        "            One-hot representation of class labels for corresponding subset\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        (X_left, y_left) : tuple of np.arrays of same type as input X_subset and y_subset\n",
        "            Part of the providev subset where selected feature x^j < threshold\n",
        "        (X_right, y_right) : tuple of np.arrays of same type as input X_subset and y_subset\n",
        "            Part of the providev subset where selected feature x^j >= threshold\n",
        "        \"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        return (X_left, y_left), (X_right, y_right)\n",
        "    \n",
        "    def make_split_only_y(self, feature_index, threshold, X_subset, y_subset):\n",
        "        \"\"\"\n",
        "        Split only target values into two subsets with specified feature and threshold\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        feature_index : int\n",
        "            Index of feature to make split with\n",
        "\n",
        "        threshold : float\n",
        "            Threshold value to perform split\n",
        "\n",
        "        X_subset : np.array of type float with shape (n_objects, n_features)\n",
        "            Feature matrix representing the selected subset\n",
        "\n",
        "        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n",
        "                   (n_objects, 1) in regression \n",
        "            One-hot representation of class labels for corresponding subset\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        y_left : np.array of type float with shape (n_objects_left, n_classes) in classification \n",
        "                   (n_objects, 1) in regression \n",
        "            Part of the provided subset where selected feature x^j < threshold\n",
        "\n",
        "        y_right : np.array of type float with shape (n_objects_right, n_classes) in classification \n",
        "                   (n_objects, 1) in regression \n",
        "            Part of the provided subset where selected feature x^j >= threshold\n",
        "        \"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        return y_left, y_right\n",
        "\n",
        "    def choose_best_split(self, X_subset, y_subset):\n",
        "        \"\"\"\n",
        "        Greedily select the best feature and best threshold w.r.t. selected criterion\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X_subset : np.array of type float with shape (n_objects, n_features)\n",
        "            Feature matrix representing the selected subset\n",
        "\n",
        "        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n",
        "                   (n_objects, 1) in regression \n",
        "            One-hot representation of class labels or target values for corresponding subset\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        feature_index : int\n",
        "            Index of feature to make split with\n",
        "\n",
        "        threshold : float\n",
        "            Threshold value to perform split\n",
        "\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        return feature_index, threshold\n",
        "    \n",
        "    def make_tree(self, X_subset, y_subset):\n",
        "        \"\"\"\n",
        "        Recursively builds the tree\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X_subset : np.array of type float with shape (n_objects, n_features)\n",
        "            Feature matrix representing the selected subset\n",
        "\n",
        "        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n",
        "                   (n_objects, 1) in regression \n",
        "            One-hot representation of class labels or target values for corresponding subset\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        root_node : Node class instance\n",
        "            Node of the root of the fitted tree\n",
        "        \"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        return new_node\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model from scratch using the provided data\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.array of type float with shape (n_objects, n_features)\n",
        "            Feature matrix representing the data to train on\n",
        "\n",
        "        y : np.array of type int with shape (n_objects, 1) in classification \n",
        "                   of type float with shape (n_objects, 1) in regression \n",
        "            Column vector of class labels in classification or target values in regression\n",
        "        \n",
        "        \"\"\"\n",
        "        assert len(y.shape) == 2 and len(y) == len(X), 'Wrong y shape'\n",
        "        self.criterion, self.classification = self.all_criterions[self.criterion_name]\n",
        "        if self.classification:\n",
        "            if self.n_classes is None:\n",
        "                self.n_classes = len(np.unique(y))\n",
        "            y = one_hot_encode(self.n_classes, y)\n",
        "\n",
        "        self.root = self.make_tree(X, y)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the target value or class label  the model from scratch using the provided data\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.array of type float with shape (n_objects, n_features)\n",
        "            Feature matrix representing the data the predictions should be provided for\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_predicted : np.array of type int with shape (n_objects, 1) in classification \n",
        "                   (n_objects, 1) in regression \n",
        "            Column vector of class labels in classification or target values in regression\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        return y_predicted\n",
        "        \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Only for classification\n",
        "        Predict the class probabilities using the provided data\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.array of type float with shape (n_objects, n_features)\n",
        "            Feature matrix representing the data the predictions should be provided for\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_predicted_probs : np.array of type float with shape (n_objects, n_classes)\n",
        "            Probabilities of each class for the provided objects\n",
        "        \n",
        "        \"\"\"\n",
        "        assert self.classification, 'Available only for classification problem'\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        return y_predicted_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P0kGCZcgCt3"
      },
      "source": [
        "## Simple check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZxqEeXmgCt3"
      },
      "source": [
        "X = np.ones((4, 5), dtype=float) * np.arange(4)[:, None]\n",
        "y = np.arange(4)[:, None] + np.asarray([0.2, -0.3, 0.1, 0.4])[:, None]\n",
        "class_estimator = DecisionTree(max_depth=10, criterion_name='gini')\n",
        "\n",
        "(X_l, y_l), (X_r, y_r) = class_estimator.make_split(1, 1., X, y)\n",
        "\n",
        "assert np.array_equal(X[:1], X_l)\n",
        "assert np.array_equal(X[1:], X_r)\n",
        "assert np.array_equal(y[:1], y_l)\n",
        "assert np.array_equal(y[1:], y_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MoJEE56gCt4"
      },
      "source": [
        "## Classification problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EloBlrsJgCt4"
      },
      "source": [
        "digits_data = load_digits().data\n",
        "digits_target = load_digits().target[:, None] # to make the targets consistent with our model interfaces\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits_data, digits_target, test_size=0.2, random_state=RANDOM_STATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKM1XAaQgCt4"
      },
      "source": [
        "assert len(y_train.shape) == 2 and y_train.shape[0] == len(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foKci4cRgCt5"
      },
      "source": [
        "class_estimator = DecisionTree(max_depth=10, criterion_name='gini')\n",
        "class_estimator.fit(X_train, y_train)\n",
        "ans = class_estimator.predict(X_test)\n",
        "accuracy_gini = accuracy_score(y_test, ans)\n",
        "print(accuracy_gini)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JncMnQ84gCt5"
      },
      "source": [
        "reference = np.array([0.09027778, 0.09236111, 0.08333333, 0.09583333, 0.11944444,\n",
        "       0.13888889, 0.09930556, 0.09444444, 0.08055556, 0.10555556])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TNrhKvvgCt5"
      },
      "source": [
        "class_estimator = DecisionTree(max_depth=10, criterion_name='entropy')\n",
        "class_estimator.fit(X_train, y_train)\n",
        "ans = class_estimator.predict(X_test)\n",
        "accuracy_entropy = accuracy_score(y_test, ans)\n",
        "print(accuracy_entropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wot6IHD2gCt5"
      },
      "source": [
        "assert  0.84 < accuracy_gini < 0.9\n",
        "assert  0.86 < accuracy_entropy < 0.9\n",
        "assert np.sum(np.abs(class_estimator.predict_proba(X_test).mean(axis=0) - reference)) < 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZi6zvDXgCt5"
      },
      "source": [
        "Let's use 5-fold cross validation (`GridSearchCV`) to find optimal values for `max_depth` and `criterion` hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BTiOQVGgCt6"
      },
      "source": [
        "param_grid = {'max_depth': range(3,11), 'criterion_name': ['gini', 'entropy']}\n",
        "gs = GridSearchCV(DecisionTree(), param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzGv0-vogCt6"
      },
      "source": [
        "%%time\n",
        "gs.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-jJOjCQgCt6"
      },
      "source": [
        "gs.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c38IvYRygCt6"
      },
      "source": [
        "assert gs.best_params_['criterion_name'] == 'entropy'\n",
        "assert 6 < gs.best_params_['max_depth'] < 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNkCBXZwgCt7"
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.title(\"The dependence of quality on the depth of the tree\")\n",
        "plt.plot(np.arange(3,11), gs.cv_results_['mean_test_score'][:8], label='Gini')\n",
        "plt.plot(np.arange(3,11), gs.cv_results_['mean_test_score'][8:], label='Entropy')\n",
        "plt.legend(fontsize=11, loc=1)\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxiCk9hQgCt7"
      },
      "source": [
        "## Regression problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm_hWALtgCt7"
      },
      "source": [
        "regr_data = load_boston().data\n",
        "regr_target = load_boston().target[:, None] # to make the targets consistent with our model interfaces\n",
        "RX_train, RX_test, Ry_train, Ry_test = train_test_split(regr_data, regr_target, test_size=0.2, random_state=RANDOM_STATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtjhzCNUgCt7"
      },
      "source": [
        "regressor = DecisionTree(max_depth=10, criterion_name='mad_median')\n",
        "regressor.fit(RX_train, Ry_train)\n",
        "predictions_mad = regressor.predict(RX_test)\n",
        "mse_mad = mean_squared_error(Ry_test, predictions_mad)\n",
        "print(mse_mad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_5Y2BehgCt8"
      },
      "source": [
        "regressor = DecisionTree(max_depth=10, criterion_name='variance')\n",
        "regressor.fit(RX_train, Ry_train)\n",
        "predictions_mad = regressor.predict(RX_test)\n",
        "mse_var = mean_squared_error(Ry_test, predictions_mad)\n",
        "print(mse_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epdeAHcJgCt8"
      },
      "source": [
        "assert 9 < mse_mad < 20\n",
        "assert 8 < mse_var < 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIyIAOGpgCt8"
      },
      "source": [
        "param_grid_R = {'max_depth': range(2,9), 'criterion_name': ['variance', 'mad_median']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFSeJHxdgCt8"
      },
      "source": [
        "gs_R = GridSearchCV(DecisionTree(), param_grid=param_grid_R, cv=5, scoring='neg_mean_squared_error', n_jobs=-2)\n",
        "gs_R.fit(RX_train, Ry_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GPiwFNfgCt8"
      },
      "source": [
        "gs_R.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCIsWmccgCt8"
      },
      "source": [
        "assert gs_R.best_params_['criterion_name'] == 'mad_median'\n",
        "assert 3 < gs_R.best_params_['max_depth'] < 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriogThCgCt8"
      },
      "source": [
        "var_scores = gs_R.cv_results_['mean_test_score'][:7]\n",
        "mad_scores = gs_R.cv_results_['mean_test_score'][7:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxtVGAJngCt9"
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.title(\"The dependence of neg_mse on the depth of the tree\")\n",
        "plt.plot(np.arange(2,9), var_scores, label='variance')\n",
        "plt.plot(np.arange(2,9), mad_scores, label='mad_median')\n",
        "plt.legend(fontsize=11, loc=1)\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.ylabel('neg_mse')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}